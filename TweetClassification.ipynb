{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"5\">Excersise 2 in Natural Language Processing</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"4\">Tweet Classification and Sentiment Analysis</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Author:</font><font color=\"#ADADAD\" size=\"3\">Angeliki Mylonaki</font>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def getData(regex):\n",
    "    trainFiles = glob.glob(os.path.join(\".\", regex)) #make list of paths\n",
    "\n",
    "    tsvreader=[]\n",
    "\n",
    "    for fd in trainFiles:\n",
    "        with open(fd) as tsvfile:\n",
    "            tsvreader.extend(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "\n",
    "    return [line[2] for line in tsvreader], [line[1] for line in tsvreader]\n",
    "            \n",
    "def peekOnData(x,y):\n",
    "    print \"Previewing data\"\n",
    "    print \".....................................\"\n",
    "    for i in range(1,10):\n",
    "        print \">> \"+ y[i] +\": \"+x[i]+\"\\n\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"#5642C2\" size=\"5\" >Tweet Classifier</font></b>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Reading +peeking on all training data</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY=getData(\"*train*.tsv\")\n",
    "devX,devY=getData(\"*dev*.tsv\")\n",
    "testX,testY=getData(\"*test*.tsv\")\n",
    "\n",
    "#peekOnData(testX[:10],testY[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Mining Tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from autocorrect import spell\n",
    "\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def mineTweets(tweetList):\n",
    "    minedList=[]\n",
    "    \n",
    "    stopWords = corpus.stopwords.words(\"english\")\n",
    "    tweetList=[s.translate(None, string.punctuation) for s in tweetList]\n",
    "    \n",
    "    for tweet in tweetList:\n",
    "        \n",
    "        #splitting tweet into phrases\n",
    "        sentences = sent_tokenize(tweet)\n",
    "        for sentence in sentences:\n",
    "            #removing digits\n",
    "            sentence=sentence.translate(None, string.digits)\n",
    "            #removing urls\n",
    "            sentence=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            #getting words of tweet phrase\n",
    "            words=word_tokenize(sentence)\n",
    "            \n",
    "            #fixing spelling \n",
    "            words=[reduce_lengthening(word) for word in words]\n",
    "\n",
    "            #Poor laptop could not handle spell check\n",
    "            #words=[spell(word) for word in words]\n",
    "\n",
    "            \n",
    "            #removing stopwords\n",
    "            sentence = ' '.join([word for word in words\n",
    "                             if word not in stopWords])\n",
    "            \n",
    "            minedList.append(sentence)\n",
    "            \n",
    "    return minedList\n",
    "\n",
    "trainX=mineTweets(trainX)\n",
    "testX=mineTweets(testX)\n",
    "devX=mineTweets(devX)\n",
    "\n",
    "#had to drop a lot of data for efficiency.\n",
    "#Feel free to comment the list slices :)\n",
    "trainX=trainX[:100]\n",
    "trainY=trainY[:100]\n",
    "testX=testX[:100]\n",
    "testY=testY[:100]\n",
    "devX=devX[:100]\n",
    "devY=devY[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Vectorizing </font><font color=\"#ADADAD\" size=\"3\">(used in textblob solution)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words=\"english\", binary=\"true\")\n",
    "X_train_counts = count_vect.fit_transform(trainX)\n",
    "X_dev_counts = count_vect.transform(devX)\n",
    "X_test_counts=  count_vect.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">TF-IDF </font><font color=\"#ADADAD\" size=\"3\">(used in textblob solution)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_dev_tfidf = tfidf_transformer.transform(X_dev_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\"> Training Multiple Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Gradient Descent:       0.44\n",
      "Random Forests:                0.49\n",
      "Multinomial Naive Bayes:       0.48\n",
      "KNN:                           0.35\n",
      "PassiveAggressive:             0.43\n",
      "LinearSVC:                     0.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "classifiers=[[SGDClassifier(),\"Linear Gradient Descent\"],\n",
    "             [RandomForestClassifier(),\"Random Forests\"],\n",
    "             [MultinomialNB(),\"Multinomial Naive Bayes\"],\n",
    "             [KNeighborsClassifier(),\"KNN\"],\n",
    "             [LinearSVC(),\"LinearSVC\"]]\n",
    "\n",
    "for clf in classifiers:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', clf[0]),\n",
    "    ])\n",
    "    text_clf.fit(trainX, trainY)  \n",
    "    predicted = text_clf.predict(testX)\n",
    "    print str(clf[1]+\": \").ljust(30),text_clf.score(testX, testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\"> Fine Tuning LinearSVC</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC tuned:               0.84\n"
     ]
    }
   ],
   "source": [
    "#tuning hyperParameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy \n",
    "\n",
    "#Using RandomizedSearch to tube SVN hyoeroaraneter C\n",
    "\n",
    "param_grid = {'C': scipy.stats.expon(scale=100), \n",
    "              'dual': [True,False],\n",
    "              'tol': scipy.stats.expon(scale=.1)}\n",
    "\n",
    "rsearch = RandomizedSearchCV(estimator=LinearSVC(), param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X_dev_tfidf,devY)\n",
    "\n",
    "predicted = rsearch.predict(X_test_tfidf)\n",
    "print str(\"LinearSVC tuned: \").ljust(30),np.mean(predicted == testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b><font color=\"#5642C2\" size=\"5\">Introducing Sentiment Analysis</font></b>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Mining Textblob Objects function definitions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.classifiers import DecisionTreeClassifier\n",
    "from textblob.classifiers import MaxEntClassifier\n",
    "\n",
    "def generateInputforClassification(x,y):\n",
    "    objList=[]\n",
    "    for i,j in zip(x,y):\n",
    "        objList.append((i,j))\n",
    "    return objList\n",
    "\n",
    "def createTextBlobs(cl,tweets):\n",
    "    objList=[]\n",
    "    for tweet in tweets:\n",
    "        twt=TextBlob(tweet,classifier=cl)\n",
    "        objList.append(twt)\n",
    "    return objList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Evaluation funtion definitions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarityDistribution(lst):\n",
    "    print \"    Negative tweets percentage: \".ljust(35),100*float(lst.count(\"negative\"))/len(lst),\"%\"\n",
    "    print \"    Neutral tweets percentage: \".ljust(35), 100*float(lst.count(\"neutral\"))/len(lst),\"%\"\n",
    "    print \"    Positive tweets percentage: \".ljust(35),100*float(lst.count(\"positive\"))/len(lst),\"%\"\n",
    "    \n",
    "def evaluateResults(clName,cl,predicted,testX,testY):\n",
    "    docs=generateInputforClassification(testX,testY)\n",
    "    predicted=calculatePolarity(textBlobs)\n",
    "\n",
    "    print \">>Classifier: \"+ clName\n",
    "    polarityDistribution(predicted)\n",
    "    print \"    Accuracy: \".ljust(35),cl.accuracy(docs),\"\\n\\n\"\n",
    "\n",
    "def summarizeOriginalData(data):\n",
    "    print \"\\n\\nActual Distribution: polarity\"\n",
    "    print \"...................................\"\n",
    "    polarityDistribution(data)\n",
    "\n",
    "def calculatePolarity(tweets):\n",
    "    predicted=[]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        if tweet.classify()==\"positive\":\n",
    "            predicted.append(\"positive\")\n",
    "        elif tweet.classify()==\"negative\":\n",
    "            predicted.append(\"negative\")\n",
    "        else:\n",
    "            predicted.append(\"neutral\")\n",
    "    \n",
    "    return predicted\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Training Algortithms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs=generateInputforClassification(trainX,trainY)\n",
    "\n",
    "classifiers=[[\"NaiveBayesClassifier\",NaiveBayesClassifier(docs)],\n",
    "             [\"DecisionTreeClassifier\",DecisionTreeClassifier(docs)],\n",
    "             [\"MaxEntClassifier\",MaxEntClassifier(docs)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Evaluating Results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Result\n",
      "...................................\n",
      ">>Classifier: NaiveBayesClassifier\n",
      "    Negative tweets percentage:     5.0 %\n",
      "    Neutral tweets percentage:      0.0 %\n",
      "    Positive tweets percentage:     95.0 %\n",
      "    Accuracy:                       0.48 \n",
      "\n",
      "\n",
      ">>Classifier: DecisionTreeClassifier\n",
      "    Negative tweets percentage:     8.0 %\n",
      "    Neutral tweets percentage:      7.0 %\n",
      "    Positive tweets percentage:     85.0 %\n",
      "    Accuracy:                       0.48 \n",
      "\n",
      "\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.520\n",
      "             2          -0.04879        1.000\n",
      "             3          -0.00385        1.000\n",
      "             4          -0.00054        1.000\n",
      "             5          -0.00010        1.000\n",
      "             6          -0.00002        1.000\n",
      "             7          -0.00000        1.000\n",
      "             8          -0.00000        1.000\n",
      "             9          -0.00000        1.000\n",
      "            10          -0.00000        1.000\n",
      "            11          -0.00000        1.000\n",
      "            12          -0.00000        1.000\n",
      "            13          -0.00000        1.000\n",
      "            14          -0.00000        1.000\n",
      "            15          -0.00000        1.000\n",
      "            16          -0.00000        1.000\n",
      "            17          -0.00000        1.000\n",
      "            18          -0.00000        1.000\n",
      "            19          -0.00000        1.000\n",
      "            20          -0.00000        1.000\n",
      "            21          -0.00000        1.000\n",
      "            22          -0.00000        1.000\n",
      "            23           0.00000        1.000\n",
      "            24           0.00000        1.000\n",
      "            25           0.00000        1.000\n",
      "            26           0.00000        1.000\n",
      "            27           0.00000        1.000\n",
      "            28           0.00000        1.000\n",
      "            29           0.00000        1.000\n",
      "            30           0.00000        1.000\n",
      "            31           0.00000        1.000\n",
      "            32           0.00000        1.000\n",
      "            33           0.00000        1.000\n",
      "            34           0.00000        1.000\n",
      "            35           0.00000        1.000\n",
      "            36           0.00000        1.000\n",
      "            37           0.00000        1.000\n",
      "            38           0.00000        1.000\n",
      "            39           0.00000        1.000\n",
      "            40           0.00000        1.000\n",
      "            41           0.00000        1.000\n",
      "            42           0.00000        1.000\n",
      "            43           0.00000        1.000\n",
      "            44           0.00000        1.000\n",
      "            45           0.00000        1.000\n",
      "            46           0.00000        1.000\n",
      "            47           0.00000        1.000\n",
      "            48           0.00000        1.000\n",
      "            49           0.00000        1.000\n",
      "            50           0.00000        1.000\n",
      "            51           0.00000        1.000\n",
      "            52           0.00000        1.000\n",
      "            53           0.00000        1.000\n",
      "            54           0.00000        1.000\n",
      "            55           0.00000        1.000\n",
      "            56           0.00000        1.000\n",
      "            57           0.00000        1.000\n",
      "            58           0.00000        1.000\n",
      "            59           0.00000        1.000\n",
      "            60           0.00000        1.000\n",
      "            61           0.00000        1.000\n",
      "            62           0.00000        1.000\n",
      "            63           0.00000        1.000\n",
      "            64           0.00000        1.000\n",
      "            65           0.00000        1.000\n",
      "            66           0.00000        1.000\n",
      "            67           0.00000        1.000\n",
      "            68           0.00000        1.000\n",
      "            69           0.00000        1.000\n",
      "            70           0.00000        1.000\n",
      "            71           0.00000        1.000\n",
      "            72           0.00000        1.000\n",
      "            73           0.00000        1.000\n",
      "            74           0.00000        1.000\n",
      "            75           0.00000        1.000\n",
      "            76           0.00000        1.000\n",
      "            77           0.00000        1.000\n",
      "            78           0.00000        1.000\n",
      "            79           0.00000        1.000\n",
      "            80           0.00000        1.000\n",
      "            81           0.00000        1.000\n",
      "            82           0.00000        1.000\n",
      "            83           0.00000        1.000\n",
      "            84           0.00000        1.000\n",
      "            85           0.00000        1.000\n",
      "            86           0.00000        1.000\n",
      "            87           0.00000        1.000\n",
      "            88           0.00000        1.000\n",
      "            89           0.00000        1.000\n",
      "            90           0.00000        1.000\n",
      "            91           0.00000        1.000\n",
      "            92           0.00000        1.000\n",
      "            93           0.00000        1.000\n",
      "            94           0.00000        1.000\n",
      "            95           0.00000        1.000\n",
      "            96           0.00000        1.000\n",
      "            97           0.00000        1.000\n",
      "            98           0.00000        1.000\n",
      "            99           0.00000        1.000\n",
      "         Final           0.00000        1.000\n",
      ">>Classifier: MaxEntClassifier\n",
      "    Negative tweets percentage:     30.0 %\n",
      "    Neutral tweets percentage:      2.0 %\n",
      "    Positive tweets percentage:     68.0 %\n",
      "    Accuracy:                       0.48 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Actual Distribution: polarity\n",
      "...................................\n",
      "    Negative tweets percentage:     10.0 %\n",
      "    Neutral tweets percentage:      40.0 %\n",
      "    Positive tweets percentage:     50.0 %\n"
     ]
    }
   ],
   "source": [
    "print \"Sentiment Analysis Result\"\n",
    "print \"...................................\"\n",
    "for cl in classifiers: \n",
    "    textBlobs=createTextBlobs(cl[1],testX)\n",
    "    evaluateResults(cl[0],cl[1],predicted,testX,testY)\n",
    "\n",
    "summarizeOriginalData(testY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
