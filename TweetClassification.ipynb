{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def getData(regex):\n",
    "    trainFiles = glob.glob(os.path.join(\".\", regex)) #make list of paths\n",
    "\n",
    "    tsvreader=[]\n",
    "\n",
    "    for fd in trainFiles:\n",
    "        with open(fd) as tsvfile:\n",
    "            tsvreader.extend(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "\n",
    "    return [line[2] for line in tsvreader], [line[1] for line in tsvreader]\n",
    "            \n",
    "def peekOnData(x,y):\n",
    "    print \"Previewing data\"\n",
    "    print \".....................................\"\n",
    "    for i in range(1,10):\n",
    "        print \">> \"+ y[i] +\": \"+x[i]+\"\\n\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Reading +peeking on all training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing data\n",
      ".....................................\n",
      ">> neutral: @fakethom Have android tab and don't use phone much, in fact very little. May go the Sony route then:-)\n",
      "\n",
      ">> positive: Finally I get my ps4 back I sent it to Sony cause my HDMI was mess up now I can play MG's Tuesday yeaaaaa buddy\n",
      "\n",
      ">> neutral: Sony's 1st teaser package for the launch of the original Playstation seems to feature a dominatrix? https://t.co/xbisCRkPL4 #MistressSophia\n",
      "\n",
      ">> neutral: #tv Ind vs SL 3rd Test Day 3: Cricket live score and Sony Six live streaming info: Watch the live teleca... http://t.co/mUlHw4cN00 #Sony\n",
      "\n",
      ">> neutral: @TruthInsider @bertymufc @gamerxone720 @PNF4LYFE @Yanks2013 @VirtuaMe Lol it's all about Sony Sony Sony, if Sony gave Bj's u be the 1st\n",
      "\n",
      ">> positive: When you remember Sony is trying to make bible study mandatory on Sunday nights @xo_taylorbang http://t.co/CfljDMvMv3\n",
      "\n",
      ">> positive: @InfinityInq Everyone is playing 3.0 and I'm just sitting here playing 2.0 because Sony is making me wait until tomorrow.\n",
      "\n",
      ">> neutral: @InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      ">> positive: @tauriqmoosa Nope. Tomorrow.  Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies.  Oh, what a LOVELY day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY=getData(\"*train*.tsv\")\n",
    "devX,devY=getData(\"*dev*.tsv\")\n",
    "testX,testY=getData(\"*test*.tsv\")\n",
    "\n",
    "peekOnData(testX[:10],testY[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Mining Tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from autocorrect import spell\n",
    "\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def mineTweets(tweetList):\n",
    "    minedList=[]\n",
    "    \n",
    "    stopWords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tweetList=[s.translate(None, string.punctuation) for s in tweetList]\n",
    "    \n",
    "    for tweet in tweetList:\n",
    "        \n",
    "        #splitting tweet into phrases\n",
    "        sentences = nltk.sent_tokenize(tweet)\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            #getting words of tweet phrase\n",
    "            words=nltk.word_tokenize(sentence)\n",
    "            \n",
    "            #fixing spelling \n",
    "            words=[reduce_lengthening(word) for word in words]\n",
    "            #should uncoment that, but it introduces HUGE DELAY\n",
    "            #words=[spell(word) for word in words]\n",
    "\n",
    "            \n",
    "            #removing stopwords\n",
    "            sentence = ' '.join([word for word in words\n",
    "                             if word not in stopWords])\n",
    "            \n",
    "            minedList.append(sentence)\n",
    "            \n",
    "    return minedList\n",
    "\n",
    "trainX=mineTweets(trainX)\n",
    "testX=mineTweets(testX)\n",
    "devX=mineTweets(devX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Vectorizing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words=\"english\", binary=\"true\")\n",
    "X_dev_counts = count_vect.fit_transform(devX)\n",
    "X_test_counts=  count_vect.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Calculating TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_dev_tfidf = tfidf_transformer.transform(X_dev_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Training Multiple Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Gradient Descent:       0.589858226955\n",
      "Random Forests:                0.589328784046\n",
      "Multinomial Naive Bayes:       0.59127007471\n",
      "KNN:                           0.591975998588\n",
      "NearestCentroid:               0.588916995117\n",
      "PassiveAggressive:             0.590564150832\n",
      "LinearSVC:                     0.59121124772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "classifiers=[[SGDClassifier(),\"Linear Gradient Descent\"],[RandomForestClassifier(),\"Random Forests\"],\n",
    "             [MultinomialNB(),\"Multinomial Naive Bayes\"],[KNeighborsClassifier(),\"KNN\"],\n",
    "             [NearestCentroid,\"NearestCentroid\"],[PassiveAggressiveClassifier(),\"PassiveAggressive\"],\n",
    "             [LinearSVC(),\"LinearSVC\"]]\n",
    "\n",
    "for clf in classifiers:\n",
    "    ext_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', clf[0]),\n",
    "    ])\n",
    "    text_clf.fit(trainX, trainY)  \n",
    "    predicted = text_clf.predict(testX)\n",
    "    print str(clf[1]+\": \").ljust(30),np.mean(predicted == testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Fine Tuning LinearSVC</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC tuned:               0.508971115948\n"
     ]
    }
   ],
   "source": [
    "#tuning hyperParameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy \n",
    "\n",
    "#Using RandomizedSearch to tube SVN hyoeroaraneter C\n",
    "\n",
    "param_grid = {'C': scipy.stats.expon(scale=100), \n",
    "              'dual': [True,False],\n",
    "              'tol': scipy.stats.expon(scale=.1)}\n",
    "\n",
    "rsearch = RandomizedSearchCV(estimator=LinearSVC(), param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X_dev_tfidf,devY)\n",
    "\n",
    "predicted = rsearch.predict(X_test_tfidf)\n",
    "print str(\"LinearSVC tuned: \").ljust(30),np.mean(predicted == testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
