{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"5\">Excersise 2 in Natural Language Processing</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"4\">Tweet Classification and Sentiment Analysis</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Author:</font><font color=\"#ADADAD\" size=\"3\">Angeliki Mylonaki</font>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def getData(regex):\n",
    "    trainFiles = glob.glob(os.path.join(\".\", regex)) #make list of paths\n",
    "\n",
    "    tsvreader=[]\n",
    "\n",
    "    for fd in trainFiles:\n",
    "        with open(fd) as tsvfile:\n",
    "            tsvreader.extend(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "\n",
    "    return [line[2] for line in tsvreader], [line[1] for line in tsvreader]\n",
    "            \n",
    "def peekOnData(x,y):\n",
    "    print \"Previewing data\"\n",
    "    print \".....................................\"\n",
    "    for i in range(1,10):\n",
    "        print \">> \"+ y[i] +\": \"+x[i]+\"\\n\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"5\">Tweet Classifier</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Reading +peeking on all training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing data\n",
      ".....................................\n",
      ">> neutral: @fakethom Have android tab and don't use phone much, in fact very little. May go the Sony route then:-)\n",
      "\n",
      ">> positive: Finally I get my ps4 back I sent it to Sony cause my HDMI was mess up now I can play MG's Tuesday yeaaaaa buddy\n",
      "\n",
      ">> neutral: Sony's 1st teaser package for the launch of the original Playstation seems to feature a dominatrix? https://t.co/xbisCRkPL4 #MistressSophia\n",
      "\n",
      ">> neutral: #tv Ind vs SL 3rd Test Day 3: Cricket live score and Sony Six live streaming info: Watch the live teleca... http://t.co/mUlHw4cN00 #Sony\n",
      "\n",
      ">> neutral: @TruthInsider @bertymufc @gamerxone720 @PNF4LYFE @Yanks2013 @VirtuaMe Lol it's all about Sony Sony Sony, if Sony gave Bj's u be the 1st\n",
      "\n",
      ">> positive: When you remember Sony is trying to make bible study mandatory on Sunday nights @xo_taylorbang http://t.co/CfljDMvMv3\n",
      "\n",
      ">> positive: @InfinityInq Everyone is playing 3.0 and I'm just sitting here playing 2.0 because Sony is making me wait until tomorrow.\n",
      "\n",
      ">> neutral: @InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      ">> positive: @tauriqmoosa Nope. Tomorrow.  Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies.  Oh, what a LOVELY day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY=getData(\"*train*.tsv\")\n",
    "devX,devY=getData(\"*dev*.tsv\")\n",
    "testX,testY=getData(\"*test*.tsv\")\n",
    "\n",
    "peekOnData(testX[:10],testY[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Mining Tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from autocorrect import spell\n",
    "\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def mineTweets(tweetList):\n",
    "    minedList=[]\n",
    "    \n",
    "    stopWords = corpus.stopwords.words(\"english\")\n",
    "    tweetList=[s.translate(None, string.punctuation) for s in tweetList]\n",
    "    \n",
    "    for tweet in tweetList:\n",
    "        \n",
    "        #splitting tweet into phrases\n",
    "        sentences = sent_tokenize(tweet)\n",
    "        for sentence in sentences:\n",
    "            #removing digits\n",
    "            sentence=sentence.translate(None, string.digits)\n",
    "            #removing urls\n",
    "            sentence=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            #getting words of tweet phrase\n",
    "            words=word_tokenize(sentence)\n",
    "            \n",
    "            #fixing spelling \n",
    "            words=[reduce_lengthening(word) for word in words]\n",
    "            #should uncomment that, but it introduces HUGE DELAY\n",
    "            #words=[spell(word) for word in words]\n",
    "\n",
    "            \n",
    "            #removing stopwords\n",
    "            sentence = ' '.join([word for word in words\n",
    "                             if word not in stopWords])\n",
    "            \n",
    "            minedList.append(sentence)\n",
    "            \n",
    "    return minedList\n",
    "\n",
    "trainX=mineTweets(trainX)\n",
    "trainX=trainX[:100]\n",
    "trainY=trainY[:100]\n",
    "testX=mineTweets(testX)\n",
    "testX=testX[:100]\n",
    "testY=testY[:100]\n",
    "devX=mineTweets(devX)\n",
    "devX=devX[:100]\n",
    "devY=devY[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Vectorizing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words=\"english\", binary=\"true\")\n",
    "X_train_counts = count_vect.fit_transform(trainX)\n",
    "X_dev_counts = count_vect.transform(devX)\n",
    "X_test_counts=  count_vect.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Calculating TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_dev_tfidf = tfidf_transformer.transform(X_dev_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Training Multiple Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gelou/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/gelou/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Gradient Descent:       0.44\n",
      "Random Forests:                0.47\n",
      "Multinomial Naive Bayes:       0.48\n",
      "KNN:                           0.35\n",
      "PassiveAggressive:             0.41\n",
      "LinearSVC:                     0.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "classifiers=[[SGDClassifier(),\"Linear Gradient Descent\"],\n",
    "             [RandomForestClassifier(),\"Random Forests\"],\n",
    "             [MultinomialNB(),\"Multinomial Naive Bayes\"],\n",
    "             [KNeighborsClassifier(),\"KNN\"],\n",
    "             [PassiveAggressiveClassifier(),\"PassiveAggressive\"],\n",
    "             [LinearSVC(),\"LinearSVC\"]]\n",
    "\n",
    "for clf in classifiers:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', clf[0]),\n",
    "    ])\n",
    "    text_clf.fit(trainX, trainY)  \n",
    "    predicted = text_clf.predict(testX)\n",
    "    print str(clf[1]+\": \").ljust(30),text_clf.score(testX, testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\"> Fine Tuning LinearSVC</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC tuned:               0.84\n"
     ]
    }
   ],
   "source": [
    "#tuning hyperParameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy \n",
    "\n",
    "#Using RandomizedSearch to tube SVN hyoeroaraneter C\n",
    "\n",
    "param_grid = {'C': scipy.stats.expon(scale=100), \n",
    "              'dual': [True,False],\n",
    "              'tol': scipy.stats.expon(scale=.1)}\n",
    "\n",
    "rsearch = RandomizedSearchCV(estimator=LinearSVC(), param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X_dev_tfidf,devY)\n",
    "\n",
    "predicted = rsearch.predict(X_test_tfidf)\n",
    "print str(\"LinearSVC tuned: \").ljust(30),np.mean(predicted == testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"#5642C2\" size=\"5\">Introducing Sentiment Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">Function to calculate polarity for all Tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculatePolarity(tweets):\n",
    "    positive=0\n",
    "    negative=0\n",
    "    neutral=0\n",
    "    predicted=[]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        print tweet.classify()\n",
    "        if tweet.sentiment.polarity>0:\n",
    "            positive+=1\n",
    "            predicted.append(\"positive\")\n",
    "            print \"positive\\n\"\n",
    "        elif tweet.sentiment.polarity<0:\n",
    "            negative+=1\n",
    "            predicted.append(\"negative\")\n",
    "            print \"negative\\n\"\n",
    "        else:\n",
    "            neutral+=1\n",
    "            predicted.append(\"neutral\")\n",
    "            print \"neutral\\n\"\n",
    "    print \"Negative tweets percentage: \".ljust(30),100*float(negative)/len(tweets),\"%\"\n",
    "    print \"Neutral tweets percentage: \".ljust(30), 100*float(neutral)/len(tweets),\"%\"\n",
    "    print \"Positive tweets percentage: \".ljust(30),100*float(positive)/len(tweets),\"%\"\n",
    "\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#cc00ff\" size=\"3\">A bit more mining</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.classifiers import DecisionTreeClassifier\n",
    "from textblob.classifiers import MaxEntClassifier\n",
    "\n",
    "\n",
    "\n",
    "def mineTweetsTextBlob(cl,tweets):\n",
    "    objList=[]\n",
    "    for tweet in tweets:\n",
    "        twt=TextBlob(tweet,classifier=cl)\n",
    "        #convertin to lower case\n",
    "        twt=twt.lower()\n",
    "\n",
    "        #attempting to correct spelling mistakes\n",
    "        twt=twt.correct()\n",
    "\n",
    "        temp=\"\"\n",
    "        for words in twt.words:\n",
    "            temp+=words.lemmatize()+\" \"\n",
    "        \n",
    "        objList.append(TextBlob(temp,classifier=cl))\n",
    "        #objList.append(twt)\n",
    "    return objList\n",
    "\n",
    "def trainClassifier(data,label):\n",
    "    tplList=[]\n",
    "    for i,j in zip(trainX,trainY):\n",
    "        tplList.append((i,j))\n",
    "    \n",
    "    return NaiveBayesClassifier(tplList)\n",
    "    #clDT=DecisionTreeClassifier(tplList)\n",
    "    #clMaxEnt=MaxEntClassifier(tplList)\n",
    "\n",
    "    \n",
    "cl=trainClassifier(trainX,trainY)\n",
    "textBlobs=mineTweetsTextBlob(cl,testX)\n",
    "predicted=calculatePolarity(textBlobs)\n",
    "\n",
    "equal=0\n",
    "not_equal=0\n",
    "for i,j in zip(predicted,trainY):\n",
    "    \n",
    "    if i==j:\n",
    "        equal+=1\n",
    "    else:\n",
    "        not_equal+=1\n",
    "print equal,not_equal\n",
    "\n",
    "\n",
    "\n",
    "print str(\"\\n\\nTextBlob accuracy:\").ljust(30),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
